{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL LANGUAGE PROCESSING WITH TEXTACY & SPACY\n",
    "\n",
    "__Spacy__ is a very high performance NLP library for doing several tasks of NLP with ease and speed. Let us explore another library built on top of __SpaCy__ called __TextaCy__.\n",
    "\n",
    "## TEXTACY\n",
    "+ Textacy is a Python library for performing higher-level natural language processing (NLP) tasks,\n",
    "built on the high-performance Spacy library.\n",
    "+ Textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, and parsed text.\n",
    "+ Uses\n",
    "    + Text preprocessing\n",
    "    + Keyword in Context\n",
    "    + Topic modeling\n",
    "    + Information Extraction\n",
    "    + Keyterm extraction,\n",
    "    + Text and Readability statistics,\n",
    "    + Emotional valence analysis,\n",
    "    + Quotation attribution\n",
    "\n",
    "### INSTALLATION\n",
    "You can install using `pip install textacy` or `conda install -c conda-forge textacy`.\n",
    "NB: In case you are having issues with installing on windows you can use conda instead of pip.\n",
    "\n",
    "### Downloading Dataset\n",
    "You can use the following command to download the `capitol_words` dataset, whcih we will use in this tutorial.\n",
    "`python -m textacy download capital_words`\n",
    "\n",
    "### FOR LANGUAGE DETECTION\n",
    "You can either use `pip install textacy[lang]` or `pip install cld2-cffi` to install the required language pack for textacy. \n",
    "\n",
    "__NOTE__: All required the package, dependencies, and add-on packs are pre-installed for this tutorial.\n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Packages\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Textacy is a Python library for performing higher-level natural language processing (NLP) tasks, \\\n",
    "built on the high-performance Spacy library. With the basics — tokenization, part-of-speech tagging, parsing \\\n",
    "— offloaded to another library, textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, \\\n",
    "and parsed text: keyterm extraction, readability statistics, emotional valence analysis, quotation attribution, \\\n",
    "and more.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Textacy is a Python library for performing higher-level natural language processing (NLP) tasks, built on the high-performance Spacy library. With the basics — tokenization, part-of-speech tagging, parsing — offloaded to another library, textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, and parsed text: keyterm extraction, readability statistics, emotional valence analysis, quotation attribution, and more.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT PREPROCESSING WITH TEXTACY\n",
    "Following methods can be used to preprocess your text data:\n",
    "\n",
    "+ `textacy.preprocess_text()`\n",
    "+ `textacy.preprocess.`\n",
    "    + Punctuation Lowercase\n",
    "    + Urls\n",
    "    + Phone numbers\n",
    "    + Currency\n",
    "    + Emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\" The best programs, are the ones written when the programmer is supposed to be working on something else.\\\n",
    "Mike bought the book for $50 although in Paris it will cost $30 dollars. Don’t document the problem, \\\n",
    "fix it.This is from https://twitter.com/codewisdom?lang=en. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The best programs  are the ones written when the programmer is supposed to be working on something else Mike bought the book for $50 although in Paris it will cost $30 dollars  Don t document the problem  fix it This is from https   twitter com codewisdom lang=en  '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing Punctuation and Uppercase\n",
    "processed_text = textacy.preprocess.remove_punct(raw_text)\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The best programs, are the ones written when the programmer is supposed to be working on something else.Mike bought the book for $50 although in Paris it will cost $30 dollars. Don’t document the problem, fix it.This is from TWITTER '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing urls\n",
    "processed_text = textacy.preprocess.replace_urls(raw_text,replace_with='TWITTER')\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The best programs, are the ones written when the programmer is supposed to be working on something else.Mike bought the book for USD50 although in Paris it will cost USD30 dollars. Don’t document the problem, fix it.This is from TWITTER '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replacing Currency Symbols\n",
    "processed_text = textacy.preprocess.replace_currency_symbols(processed_text,replace_with='USD')\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The best programs  are the ones written when the programmer is supposed to be working on something else Mike bought the book for USD50 although in Paris it will cost USD30 dollars  Don t document the problem  fix it This is from TWITTER '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KEVIN: what should be the correct steps here of preprocessing?\n",
    "#one possible rule: pick steps with minimal impact first...\n",
    "#of course, not always simple\n",
    "#always remember what is input and what is output\n",
    "\n",
    "#find logical dependencies within txt: does one 'aspect' depend on another 'aspect', like url depends on puncuation\n",
    "\n",
    "#corrected sequence\n",
    "processed_text = textacy.preprocess.replace_currency_symbols(raw_text,replace_with='USD')\n",
    "processed_text = textacy.preprocess.replace_urls(processed_text,replace_with='TWITTER')\n",
    "processed_text = textacy.preprocess.remove_punct(processed_text)\n",
    "\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we created a variable `processed_text` in every cell block above? That is because that usually text preprocessing is a pipeline - with multiple steps in it. Here are the steps we completed above:\n",
    "+ Removing Punctuation and Uppercase\n",
    "+ Removing urls\n",
    "+ Replacing Currency Symbols\n",
    "\n",
    "So we are using the variable to pass text data from each step to the next.\n",
    "\n",
    "There are much more text preprocessing steps. [Here](https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html) is a good summary of these steps.\n",
    "\n",
    "You can incorporate all above steps together using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the best programs are the ones written when the programmer is supposed to be working on something else mike bought the book for usd50 although in paris it will cost usd30 dollars don t document the problem fix it this is from url'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess All\n",
    "processed_text = textacy.preprocess_text(raw_text,lowercase=True,no_punct=True,no_urls=True, no_currency_symbols=True)\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [docs](https://chartbeat-labs.github.io/textacy/api_reference.html#module-textacy.preprocess) for more details of the `textacy.preprocess` and its family methods.\n",
    "\n",
    "### READING A TEXT OR A DOCUMENT\n",
    "+ `textacy.Doc(your_text)`\n",
    "+ `textacy.io.read_text(your_text)`\n",
    "\n",
    "Textacy would not receive a lot of attractions if it only can remove URLs or punctuations; however, all additional/more advanced techniques/analyses required on `formatting` the data.\n",
    "\n",
    "TextaCy/SpaCy uses a `Doc` as a container for any text objects. [Here](https://chartbeat-labs.github.io/textacy/getting_started/quickstart.html#make-a-doc) is nice documentation of the `doc` object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Doc\n",
    "# Requires Language Pkg Model\n",
    "docx_textacy = textacy.Doc(example, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc(82 tokens; \"Textacy is a Python library for performing high...\")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docx_textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the `type` of the `docx_textacy` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textacy.doc.Doc"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docx_textacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code read a `doc` from a local file:\n",
    "\n",
    "1. use the .read() method\n",
    "`file_textacy = textacy.Doc(open(\"example.txt\").read())`\n",
    "\n",
    "2. create a generator\n",
    "`file_textacy2 = textacy.io.read_text('example.txt',lines=True)`\n",
    "\n",
    "then:\n",
    "\n",
    "`for text in file_textacy2:`\n",
    "\n",
    "    `docx_file = textacy.Doc(text)`\n",
    "    \n",
    "    `print(docx_file)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Text Analytics \n",
    "\n",
    "1. Named-Entity Recognition\n",
    "\n",
    "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. [Source: [Wikipedia](https://en.wikipedia.org/wiki/Named-entity_recognition)]\n",
    "\n",
    "TextaCy has a built-in method for NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NLP, Spacy]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Textacy Named Entity Extraction\n",
    "list(textacy.extract.named_entities(docx_textacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. n-grams\n",
    "\n",
    "In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.\n",
    "\n",
    "TextaCy has a built-in method for n-grams.\n",
    "\n",
    "N-grams, a.k.a __Bag-of-Words__, is a very important quantifying approach for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[library for performing,\n",
       " level natural language,\n",
       " natural language processing,\n",
       " performance Spacy library,\n",
       " With the basics,\n",
       " focuses on tasks,\n",
       " availability of tokenized,\n",
       " emotional valence analysis]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NGrams with Textacy\n",
    "# NB SpaCy method would be to use noun Phrases\n",
    "# Tri Grams\n",
    "\n",
    "list(textacy.extract.ngrams(docx_textacy,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. text statistics\n",
    "This usually includes computing basic counts and various readability statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = textacy.TextStats(docx_textacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique words\n",
    "ts.n_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_sents': 2,\n",
       " 'n_words': 60,\n",
       " 'n_chars': 364,\n",
       " 'n_syllables': 116,\n",
       " 'n_unique_words': 51,\n",
       " 'n_long_words': 28,\n",
       " 'n_monosyllable_words': 29,\n",
       " 'n_polysyllable_words': 19}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic counts of linguistic units\n",
    "ts.basic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flesch_kincaid_grade_level': 18.923333333333336,\n",
       " 'flesch_reading_ease': 12.825000000000045,\n",
       " 'smog_index': 20.736966565827903,\n",
       " 'gunning_fog_index': 24.66666666666667,\n",
       " 'coleman_liau_index': 18.884049400000002,\n",
       " 'automated_readability_index': 22.144,\n",
       " 'lix': 76.66666666666666,\n",
       " 'gulpease_index': 38.333333333333336,\n",
       " 'wiener_sachtextformel': 14.740666666666666}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# readability scores\n",
    "ts.readability_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these basic counts and readability stats seem intimidating. Feel free to Google them to better understand them. \n",
    "\n",
    "4. Dealing with a collection of documents (corpus)\n",
    "\n",
    "Many NLP tasks require datasets comprised of a large number of texts, which are often stored on disk in one or multiple files. textacy makes it easy to efficiently stream text and (text, metadata) pairs from disk, regardless of the format or compression of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '2001-02-13',\n",
       " 'congress': 107,\n",
       " 'speaker_name': 'Hillary Clinton',\n",
       " 'speaker_party': 'D',\n",
       " 'title': 'MORNING BUSINESS',\n",
       " 'text': 'I yield myself 15 minutes of the time controlled by the Democrats.',\n",
       " 'chamber': 'Senate'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textacy.datasets  # note the import\n",
    "ds = textacy.datasets.CapitolWords()\n",
    "ds.download()\n",
    "records = ds.records(speaker_name={\"Hillary Clinton\", \"Barack Obama\"})\n",
    "next(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `textacy.Corpus` is an ordered collection of spaCy Doc s, all processed by the same language pipeline. Let’s continue with the Capitol Words dataset and make a corpus from a stream of records. (Note: This may take a few minutes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus(100 docs; 70500 tokens)\n"
     ]
    }
   ],
   "source": [
    "cw = textacy.datasets.CapitolWords()\n",
    "records = cw.records(limit=100)\n",
    "text_stream, metadata_stream = textacy.io.split_records(records, 'text')\n",
    "corpus = textacy.Corpus('en', texts=text_stream, metadatas=metadata_stream)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernie Sanders\n",
      "Lindsey Graham\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "Joseph Biden\n",
      "Bernie Sanders\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Bernie Sanders\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "John Kasich\n",
      "John Kasich\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Bernie Sanders\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "Joseph Biden\n",
      "Lindsey Graham\n",
      "Rick Santorum\n",
      "Bernie Sanders\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Rick Santorum\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "John Kasich\n",
      "John Kasich\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Rick Santorum\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Lindsey Graham\n",
      "Bernie Sanders\n",
      "Rick Santorum\n",
      "Rick Santorum\n",
      "John Kasich\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Rick Santorum\n",
      "John Kasich\n",
      "Joseph Biden\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "John Kasich\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Joseph Biden\n",
      "Bernie Sanders\n",
      "Bernie Sanders\n",
      "John Kasich\n",
      "Joseph Biden\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus:\n",
    "    print(doc.metadata.get(\"speaker_name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can filter the corpus using certain conditions, which would cover your specific use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc(159 tokens; \"Mr. Speaker, 480,000 Federal employees are work...\")\n",
      "Doc(336 tokens; \"Mr. Speaker, I thank the gentleman for yielding...\")\n",
      "Doc(177 tokens; \"Mr. Speaker, if we want to understand why in th...\")\n"
     ]
    }
   ],
   "source": [
    "# Suppose we just want speeches of Mr. Bernie Sanders\n",
    "# Just print out top-3 for illustration\n",
    "match_func = lambda doc: doc.metadata.get(\"speaker_name\") == \"Bernie Sanders\"\n",
    "\n",
    "for doc in corpus.get(match_func, limit=3):\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus is a list-like object that can be iterated on - each element in Corpus is a `textacy.Doc` object. \n",
    "\n",
    "Which means we can do slicing as we slice any list in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc(159 tokens; \"Mr. Speaker, 480,000 Federal employees are work...\")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# any element\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Doc(159 tokens; \"Mr. Speaker, 480,000 Federal employees are work...\"),\n",
       " Doc(219 tokens; \"Mr. Speaker, a relationship, to work and surviv...\"),\n",
       " Doc(336 tokens; \"Mr. Speaker, I thank the gentleman for yielding...\")]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a sub-list\n",
    "[doc for doc in corpus[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Corpus(90 docs; 67161 tokens)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can delete elements from `corpus`\n",
    "del corpus[:10]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get basic statistics of the `corpus` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 2943, 67161)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.n_docs, corpus.n_sents, corpus.n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Counts - as a dictionary\n",
    "counts = corpus.word_freqs(weighting='count', as_strings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-PRON-', 4726),\n",
       " ('people', 241),\n",
       " ('mr.', 240),\n",
       " ('president', 216),\n",
       " ('bill', 209)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can get the top-5 frequent words form the `counts` dictionary\n",
    "sorted(counts.items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also introduce a new text metric called __term frequency - inversed document frequency__ (`tf-idf`). \n",
    "\n",
    "In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf–idf.[Source: Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "Term frequency (`tf`) in `tf-idf` is the word frequency we get from above dictionary (`counts`). Now we need to calculate the `idf` part. \n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides, i.e., if it's common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "idf(t, D) = log(\\frac{N_D}{N_t})\n",
    "\\end{equation*}\n",
    "\n",
    "In which, $ N_D $ is the number of documents (`doc`) in `corpus` D; and $ N_t $ is the number of `doc`s in $ D $ containing term $ t $.\n",
    "\n",
    "Looks complicated, right? Fortunately we can use TextaCy's built-in methods to calculate `idf`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = corpus.word_doc_freqs(as_strings=True, weighting='idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('355', 4.51085950651685),\n",
       " ('absent', 4.51085950651685),\n",
       " ('unavoidably', 4.51085950651685),\n",
       " ('ordering', 4.51085950651685),\n",
       " ('con', 4.51085950651685)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(idf.items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since now we have both __tf__ as a dict object `counts`; and idf as a dict object `idf`, we can calculate the complete __tf-idf__ metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = {k: counts[k]/idf[k] for k in idf.keys() & counts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "Please print out the top-20 terms with the highest `tf-idf` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-PRON-', 6654.073638294795),\n",
       " ('mr.', 295.9564154851718),\n",
       " ('president', 222.81811001485437),\n",
       " ('people', 204.4703503128777),\n",
       " ('year', 172.23021208927045),\n",
       " ('bill', 172.09519917661507),\n",
       " ('think', 126.47630887391287),\n",
       " (\"'s\", 121.62794885731749),\n",
       " ('work', 118.93710442204127),\n",
       " ('american', 106.85316269251574),\n",
       " ('know', 102.90929479802848),\n",
       " ('$', 99.25286446516297),\n",
       " ('money', 92.74811562385676),\n",
       " ('good', 92.7279409085042),\n",
       " ('want', 90.44774564026228),\n",
       " ('time', 88.60415161642001),\n",
       " ('federal', 84.80466267593039),\n",
       " ('let', 83.60715983553656),\n",
       " ('percent', 79.19058576140583),\n",
       " ('state', 77.65646307702684)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Complete your code here\n",
    "sorted(tf_idf.items(), key=lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of above results make sense, such as 'president' and 'bill' and 'act'. But terms like '-PRON-' (referring to pronouns such as 'you' or 'I') and ''s' do not make sense. Similar conclusion can be drawn onto words such as 'a', 'an', 'the', ...\n",
    "\n",
    "These words are called __stop words__. In computing, stop words are words which are filtered out before or after processing of natural language data (text). Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search. [Source: Wikipedia](https://en.wikipedia.org/wiki/Stop_words).\n",
    "\n",
    "We can check if a word (a.k.a. token) is stop by using the `is_stop` attribute provided with `textacy.token` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. False\n",
      "Speaker False\n",
      ", False\n",
      "I False\n",
      "was True\n",
      "unavoidably False\n",
      "absent False\n",
      "during True\n",
      "the True\n",
      "votes False\n",
      "on True\n",
      "default False\n",
      "legislation False\n",
      ". False\n",
      "If False\n",
      "I False\n",
      "had True\n",
      "been True\n",
      "present False\n",
      ", False\n",
      "I False\n",
      "would True\n",
      "have True\n",
      "voted False\n",
      "\" False\n",
      "nay False\n",
      "\" False\n",
      "on True\n",
      "the True\n",
      "motions False\n",
      "to True\n",
      "table False\n",
      "the True\n",
      "appeal False\n",
      "of True\n",
      "the True\n",
      "ruling False\n",
      "of True\n",
      "the True\n",
      "Chair False\n",
      "with True\n",
      "regards False\n",
      "to True\n",
      "the True\n",
      "resolutions False\n",
      "offered False\n",
      "by True\n",
      "Mr. False\n",
      "Gephardt False\n",
      "( False\n",
      "rollcall False\n",
      "No False\n",
      ". False\n",
      "26 False\n",
      ") False\n",
      "and True\n",
      "Ms. False\n",
      "Jackson False\n",
      "- False\n",
      "Lee False\n",
      "( False\n",
      "rollcall False\n",
      "No False\n",
      ". False\n",
      "27 False\n",
      ") False\n",
      ", False\n",
      "I False\n",
      "would True\n",
      "have True\n",
      "voted False\n",
      "\" False\n",
      "nay False\n",
      "\" False\n",
      "on True\n",
      "the True\n",
      "ordering False\n",
      "of True\n",
      "the True\n",
      "previous False\n",
      "question False\n",
      "on True\n",
      "House False\n",
      "Resolution False\n",
      "355 False\n",
      "( False\n",
      "rollcall False\n",
      "No False\n",
      ". False\n",
      "28 False\n",
      ") False\n",
      ". False\n",
      "I False\n",
      "would True\n",
      "have True\n",
      "voted False\n",
      "\" False\n",
      "nay False\n",
      "\" False\n",
      "on True\n",
      "H. False\n",
      "Con False\n",
      ". False\n",
      "Res False\n",
      ". False\n",
      "141 False\n",
      "( False\n",
      "rollcall False\n",
      "No False\n",
      ". False\n",
      "29 False\n",
      ") False\n",
      ". False\n",
      "I False\n",
      "would True\n",
      "have True\n",
      "voted False\n",
      "\" False\n",
      "yea False\n",
      "\" False\n",
      "on True\n",
      "H.R. False\n",
      "2924 False\n",
      "( False\n",
      "rollcall False\n",
      "No False\n",
      ". False\n",
      "30 False\n",
      ") False\n",
      ". False\n"
     ]
    }
   ],
   "source": [
    "my_doc = corpus[0]\n",
    "\n",
    "for t in my_doc.tokens:\n",
    "    print(t, t.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "Remove all stop words from `my_doc`.\n",
    "\n",
    "__HINT__: using an `if` statement (with the `is_stop` attribute) in the `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Mr.,\n",
       " Speaker,\n",
       " ,,\n",
       " I,\n",
       " unavoidably,\n",
       " absent,\n",
       " votes,\n",
       " default,\n",
       " legislation,\n",
       " .,\n",
       " If,\n",
       " I,\n",
       " present,\n",
       " ,,\n",
       " I,\n",
       " voted,\n",
       " \",\n",
       " nay,\n",
       " \",\n",
       " motions,\n",
       " table,\n",
       " appeal,\n",
       " ruling,\n",
       " Chair,\n",
       " regards,\n",
       " resolutions,\n",
       " offered,\n",
       " Mr.,\n",
       " Gephardt,\n",
       " (,\n",
       " rollcall,\n",
       " No,\n",
       " .,\n",
       " 26,\n",
       " ),\n",
       " Ms.,\n",
       " Jackson,\n",
       " -,\n",
       " Lee,\n",
       " (,\n",
       " rollcall,\n",
       " No,\n",
       " .,\n",
       " 27,\n",
       " ),\n",
       " ,,\n",
       " I,\n",
       " voted,\n",
       " \",\n",
       " nay,\n",
       " \",\n",
       " ordering,\n",
       " previous,\n",
       " question,\n",
       " House,\n",
       " Resolution,\n",
       " 355,\n",
       " (,\n",
       " rollcall,\n",
       " No,\n",
       " .,\n",
       " 28,\n",
       " ),\n",
       " .,\n",
       " I,\n",
       " voted,\n",
       " \",\n",
       " nay,\n",
       " \",\n",
       " H.,\n",
       " Con,\n",
       " .,\n",
       " Res,\n",
       " .,\n",
       " 141,\n",
       " (,\n",
       " rollcall,\n",
       " No,\n",
       " .,\n",
       " 29,\n",
       " ),\n",
       " .,\n",
       " I,\n",
       " voted,\n",
       " \",\n",
       " yea,\n",
       " \",\n",
       " H.R.,\n",
       " 2924,\n",
       " (,\n",
       " rollcall,\n",
       " No,\n",
       " .,\n",
       " 30,\n",
       " ),\n",
       " .]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Complete your code here    \n",
    "my_doc_final = [character for character in my_doc.tokens if (not character.is_stop)]\n",
    "my_doc_final\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we only care about __word tokens__, which means we need to filter out _numbers_, _punctuations_, and so forth. \n",
    "\n",
    "TextaCy provides a `is_alpha` attribute for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in my_doc.tokens:\n",
    "    print(t, t.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "What if we want non-stop and word tokens?\n",
    "\n",
    "__HINT__: combine the two above steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Speaker,\n",
       " I,\n",
       " unavoidably,\n",
       " absent,\n",
       " votes,\n",
       " default,\n",
       " legislation,\n",
       " If,\n",
       " I,\n",
       " present,\n",
       " I,\n",
       " voted,\n",
       " nay,\n",
       " motions,\n",
       " table,\n",
       " appeal,\n",
       " ruling,\n",
       " Chair,\n",
       " regards,\n",
       " resolutions,\n",
       " offered,\n",
       " Gephardt,\n",
       " rollcall,\n",
       " No,\n",
       " Jackson,\n",
       " Lee,\n",
       " rollcall,\n",
       " No,\n",
       " I,\n",
       " voted,\n",
       " nay,\n",
       " ordering,\n",
       " previous,\n",
       " question,\n",
       " House,\n",
       " Resolution,\n",
       " rollcall,\n",
       " No,\n",
       " I,\n",
       " voted,\n",
       " nay,\n",
       " Con,\n",
       " Res,\n",
       " rollcall,\n",
       " No,\n",
       " I,\n",
       " voted,\n",
       " yea,\n",
       " rollcall,\n",
       " No]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Complete you code here\n",
    "[character for character in my_doc if ((not character.is_stop) and (character.is_alpha))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, have you noticed that different forms of the same token may appear in the text? For instance, 'run', 'ran', and 'running' are all different forms of the root word 'run'. Counting them as different words may bias any subsequent model. Thus, it would be ideal to make different forms of the same word to the root. This process is called __lemmatization__.\n",
    "\n",
    "_Lemmatization_ usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the _lemma_. If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source. [Source: Stanford NLP Group](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "TextaCy provides a `lemma_` attribute for `token` object for exactly this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in my_doc.tokens:\n",
    "    print(t, t.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "In this exercise, you are going to complete following tasks.\n",
    "1. From the `corpus` variable, generate a new list named `jb_lst` that contains all speeches from `Joseph Biden`. (__HINT__: use similar code as we filter `cw` for 'Bernie Sanders'.)\n",
    "2. Select first 5 speeches (`doc`) from `jb_lst` and store them in a new list named `jb_selected`.\n",
    "3. For each element in `jb_selected`, print out:\n",
    "    a. Named Entities (`named_entities()`)\n",
    "    b. Text Statistics (`TextStats()`)\n",
    "4. For each element in `jb_selected`, print out the top 20 words based on their tf-idf score.\n",
    "5. For each element in `jb_selected`, print out __lemmas__ (`lemma_`) for each token if it is not a stop word (`is_stop`) and is a word token (`is_alpha`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[today, tens of thousands, American, hundreds of millions, American, February 19, Russian Government, American, Russia, next few weeks, American, Russia, more than $700 million, over 20 percent, American, Russia, Delmarva Peninsula, 21,000 poultry, more than 600 million, Russian, Last summer, Allen, Family Food, Seaford, DE, 1,300 tons, Russia, one, Delaware, Port of Wilmington, Delaware, Russian, Russian, Manor Farms, Allen's Foods, Delaware, 100 percent, Russia, American, day, recent years, Russia, Moscow, St. Petersburg, America, America, Russia, Russian, Russian, Russian, Russian, American, Delaware, Russian, Now, American, Russian, Russian, United States, Soviet Union, election year, Russian Government's, American, Agriculture, Dan Glickman, Russian Government, United States, IMF, Russia, $10.2 billion, Communist, Russians, Russians, monthly, 3 years, Russian, Russia, World Trade Organization and America, WTO, United States, WTO, Russia, Russians, American, WTO, up to a year, Clinton, Russians, Russian Government]\n",
      "<generator object named_entities at 0x7f7c78351eb8> 449\n",
      "[week, United States, year, U.S., U.S., U.S., U.S., hundreds of tons, American, American, America, U.S.\n",
      ", America, United States, Latin America, America, Colombia, United States, Colombia, Colombian, Mexico, U.S., last year, Colombia, Mexican, Colombia, Colombian, National Police, Serrano, Hundreds, Colombians, last year, Government, Colombia, Ernesto Samper, $6 million, Cali, at least 20, years, 1989, Bush, Bahamas, Bahamas, Government, Colombia, Senate, first, Samper, Cali, summer of 1994, U.S., Samper, House of Representatives, Samper, August 1994, United States, Colombia, Samper, Last year's, Colombia, first, United States, Colombia, Colombian, DEA, six, Cali, just last month, one, Colombian, 24 percent, last year, Colombian, America, coming year, Colombian Government, United States, Foreign Assistance Act, Colombia, Mexico, Colombia, Mexican, United States, Mexico, Mexico, year, Mexico, last year, Colombia, Mexico, Caribbean, Mexico, Mexican, Mexican, Colombian, Mexican, United States, more than two-thirds, Mexico, Mexican, Mexican, one, Mexico, Last year, Mexico, $10 million, hundreds of millions of dollars, Mexico, Mexico, Zedillo, Juan Abrego — leader, Mexican, United States, Mexican, Mexico, Mexico, Mexico, Mexico, Mexican, United States, Mexico, Last year, Colombian, year, Mexican, Mexico, Mexico, Mexican, 1993, North American Free Trade Agreement —, Mexico, last year, Clinton, Mexico, Mexico, NAFTA, Mexican, Colombia, Mexico, Colombia, United States]\n",
      "<generator object named_entities at 0x7f7c78351eb8> 712\n",
      "[past, Sunday, Israel, 23, Israelis, 2, Americans, Israelis, Sunday, Middle East, quarters, Palestinians, days, Arab, Israeli, Israel, Palestinians, Israel, Palestinians, Israelis, Israeli, Palestinians, Palestinian, Sunday, two, American, Matthew Eisenfeld, Sarah Duker, Sunday]\n",
      "<generator object named_entities at 0x7f7c78351eb8> 233\n",
      "[Hatch, Shelby, Grassley, $3.9 million, Drug, — General, McCaffrey — with, Office of National Drug Control Policy, more than a decade, Reagan, 1988, Drug Office, first, Drug Office, Cabinet, today, Drug, General, additional $3.9 million, 80, Today, Republican, McCaffrey, Republican, McCaffrey]\n",
      "<generator object named_entities at 0x7f7c78351eb8> 142\n",
      "[today, January of this year, State, Delaware, one, today, Delaware, Delaware, first, State, Constitution, Delaware, one, Senate, today, today, two, 4.25, 5.15, Delaware, two, 4.25, 5.00, State, 30,000, Delawareans, 9.5 percent, 11.5 percent, Delaware, today, 1991, Bush, nearly 50 cents, 40-year, year, full 40-hour workweek, 52 weeks, just $8,840, three-quarters, three, American, 5 percent, year, 1979, about 2.5 percent, same time.\n",
      ", Between 1977 and 1992, 20 percent, American, 17 percent, 20 percent, 28-percent, 1 percent, 91 percent, Now, last 20 to 25 years, 1 percent, Nation, America, World War \n",
      ", American, Americans, Americans, Americans, America, end of the day, century, today, Delaware, America]\n",
      "<generator object named_entities at 0x7f7c78351eb8> 433\n"
     ]
    }
   ],
   "source": [
    "#### Complete your code here\n",
    "\n",
    "#1\n",
    "jb_lst = lambda doc: doc.metadata.get(\"speaker_name\") == \"Joseph Biden\"\n",
    "\n",
    "#2\n",
    "jb_selected = [doc for doc in corpus.get(jb_lst, limit=5)]\n",
    "\n",
    "#3\n",
    "\n",
    "for element in jb_selected:\n",
    "    print(list(textacy.extract.named_entities(element)))\n",
    "    ts = textacy.TextStats(element)\n",
    "    print(entities, ts.n_unique_words)\n",
    "\n",
    "#4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'word_doc_freqs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-b0b346163e65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#4:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjb_selected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjb_selected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_doc_freqs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_strings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'idf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'word_doc_freqs'"
     ]
    }
   ],
   "source": [
    "#4:\n",
    "for element in jb_selected:\n",
    "    idf = jb_selected.word_doc_freqs(as_strings=True, weighting='idf')\n",
    "    tf_idf = {k: counts[k]/idf[k] for k in idf.keys() & counts}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of part 1. We will resume on text analytics (NLP) after the break."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
