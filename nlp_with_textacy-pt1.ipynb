{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NATURAL LANGUAGE PROCESSING WITH TEXTACY & SPACY\n",
    "\n",
    "__Spacy__ is a very high performance NLP library for doing several tasks of NLP with ease and speed. Let us explore another library built on top of __SpaCy__ called __TextaCy__.\n",
    "\n",
    "## TEXTACY\n",
    "+ Textacy is a Python library for performing higher-level natural language processing (NLP) tasks,\n",
    "built on the high-performance Spacy library.\n",
    "+ Textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, and parsed text.\n",
    "+ Uses\n",
    "    + Text preprocessing\n",
    "    + Keyword in Context\n",
    "    + Topic modeling\n",
    "    + Information Extraction\n",
    "    + Keyterm extraction,\n",
    "    + Text and Readability statistics,\n",
    "    + Emotional valence analysis,\n",
    "    + Quotation attribution\n",
    "\n",
    "### INSTALLATION\n",
    "You can install using `pip install textacy` or `conda install -c conda-forge textacy`.\n",
    "NB: In case you are having issues with installing on windows you can use conda instead of pip.\n",
    "\n",
    "### Downloading Dataset\n",
    "You can use the following command to download the `capitol_words` dataset, whcih we will use in this tutorial.\n",
    "`python -m textacy download capital_words`\n",
    "\n",
    "### FOR LANGUAGE DETECTION\n",
    "You can either use `pip install textacy[lang]` or `pip install cld2-cffi` to install the required language pack for textacy. \n",
    "\n",
    "__NOTE__: All required the package, dependencies, and add-on packs are pre-installed for this tutorial.\n",
    "\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Packages\n",
    "import textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Textacy is a Python library for performing higher-level natural language processing (NLP) tasks, \\\n",
    "built on the high-performance Spacy library. With the basics — tokenization, part-of-speech tagging, parsing \\\n",
    "— offloaded to another library, textacy focuses on tasks facilitated by the availability of tokenized, POS-tagged, \\\n",
    "and parsed text: keyterm extraction, readability statistics, emotional valence analysis, quotation attribution, \\\n",
    "and more.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT PREPROCESSING WITH TEXTACY\n",
    "Following methods can be used to preprocess your text data:\n",
    "\n",
    "+ `textacy.preprocess_text()`\n",
    "+ `textacy.preprocess.`\n",
    "    + Punctuation Lowercase\n",
    "    + Urls\n",
    "    + Phone numbers\n",
    "    + Currency\n",
    "    + Emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\" The best programs, are the ones written when the programmer is supposed to be working on something else.\\\n",
    "Mike bought the book for $50 although in Paris it will cost $30 dollars. Don’t document the problem, \\\n",
    "fix it.This is from https://twitter.com/codewisdom?lang=en. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Punctuation and Uppercase\n",
    "processed_text = textacy.preprocess.remove_punct(raw_text)\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing urls\n",
    "processed_text = textacy.preprocess.replace_urls(processed_text,replace_with='TWITTER')\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing Currency Symbols\n",
    "processed_text = textacy.preprocess.replace_currency_symbols(processed_text,replace_with='USD')\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we created a variable `processed_text` in every cell block above? That is because that usually text preprocessing is a pipeline - with multiple steps in it. Here are the steps we completed above:\n",
    "+ Removing Punctuation and Uppercase\n",
    "+ Removing urls\n",
    "+ Replacing Currency Symbols\n",
    "\n",
    "So we are using the variable to pass text data from each step to the next.\n",
    "\n",
    "There are much more text preprocessing steps. [Here](https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html) is a good summary of these steps.\n",
    "\n",
    "You can incorporate all above steps together using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess All\n",
    "processed_text = textacy.preprocess_text(raw_text,lowercase=True,no_punct=True,no_urls=True)\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the [docs](https://chartbeat-labs.github.io/textacy/api_reference.html#module-textacy.preprocess) for more details of the `textacy.preprocess` and its family methods.\n",
    "\n",
    "### READING A TEXT OR A DOCUMENT\n",
    "+ `textacy.Doc(your_text)`\n",
    "+ `textacy.io.read_text(your_text)`\n",
    "\n",
    "Textacy would not receive a lot of attractions if it only can remove URLs or punctuations; however, all additional/more advanced techniques/analyses required on `formatting` the data.\n",
    "\n",
    "TextaCy/SpaCy uses a `Doc` as a container for any text objects. [Here](https://chartbeat-labs.github.io/textacy/getting_started/quickstart.html#make-a-doc) is nice documentation of the `doc` object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Doc\n",
    "# Requires Language Pkg Model\n",
    "docx_textacy = textacy.Doc(example, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the `type` of the `docx_textacy` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(docx_textacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code read a `doc` from a local file:\n",
    "\n",
    "1. use the .read() method\n",
    "`file_textacy = textacy.Doc(open(\"example.txt\").read())`\n",
    "\n",
    "2. create a generator\n",
    "`file_textacy2 = textacy.io.read_text('example.txt',lines=True)`\n",
    "\n",
    "then:\n",
    "\n",
    "`for text in file_textacy2:`\n",
    "\n",
    "    `docx_file = textacy.Doc(text)`\n",
    "    \n",
    "    `print(docx_file)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Text Analytics \n",
    "\n",
    "1. Named-Entity Recognition\n",
    "\n",
    "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. [Source: [Wikipedia](https://en.wikipedia.org/wiki/Named-entity_recognition)]\n",
    "\n",
    "TextaCy has a built-in method for NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Textacy Named Entity Extraction\n",
    "list(textacy.extract.named_entities(docx_textacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. n-grams\n",
    "\n",
    "In the fields of computational linguistics and probability, an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.\n",
    "\n",
    "TextaCy has a built-in method for n-grams.\n",
    "\n",
    "N-grams, a.k.a __Bag-of-Words__, is a very important quantifying approach for text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NGrams with Textacy\n",
    "# NB SpaCy method would be to use noun Phrases\n",
    "# Tri Grams\n",
    "\n",
    "list(textacy.extract.ngrams(docx_textacy,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. text statistics\n",
    "This usually includes computing basic counts and various readability statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = textacy.TextStats(docx_textacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique words\n",
    "ts.n_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic counts of linguistic units\n",
    "ts.basic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# readability scores\n",
    "ts.readability_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these basic counts and readability stats seem intimidating. Feel free to Google them to better understand them. \n",
    "\n",
    "4. Dealing with a collection of documents (corpus)\n",
    "\n",
    "Many NLP tasks require datasets comprised of a large number of texts, which are often stored on disk in one or multiple files. textacy makes it easy to efficiently stream text and (text, metadata) pairs from disk, regardless of the format or compression of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.datasets  # note the import\n",
    "ds = textacy.datasets.CapitolWords()\n",
    "ds.download()\n",
    "records = ds.records(speaker_name={\"Hillary Clinton\", \"Barack Obama\"})\n",
    "next(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `textacy.Corpus` is an ordered collection of spaCy Doc s, all processed by the same language pipeline. Let’s continue with the Capitol Words dataset and make a corpus from a stream of records. (Note: This may take a few minutes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = textacy.datasets.CapitolWords()\n",
    "records = cw.records(limit=100)\n",
    "text_stream, metadata_stream = textacy.io.split_records(records, 'text')\n",
    "corpus = textacy.Corpus('en', texts=text_stream, metadatas=metadata_stream)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for doc in corpus:\n",
    "    print(doc.metadata.get(\"speaker_name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can filter the corpus using certain conditions, which would cover your specific use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Suppose we just want speeches of Mr. Bernie Sanders\n",
    "# Just print out top-3 for illustration\n",
    "match_func = lambda doc: doc.metadata.get(\"speaker_name\") == \"Bernie Sanders\"\n",
    "\n",
    "for doc in corpus.get(match_func, limit=3):\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus is a list-like object that can be iterated on - each element in Corpus is a `textacy.Doc` object. \n",
    "\n",
    "Which means we can do slicing as we slice any list in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any element\n",
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sub-list\n",
    "[doc for doc in corpus[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can delete elements from `corpus`\n",
    "del corpus[:10]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get basic statistics of the `corpus` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.n_docs, corpus.n_sents, corpus.n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Counts - as a dictionary\n",
    "counts = corpus.word_freqs(weighting='count', as_strings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get the top-5 frequent words form the `counts` dictionary\n",
    "sorted(counts.items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also introduce a new text metric called __term frequency - inversed document frequency__ (`tf-idf`). \n",
    "\n",
    "In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf–idf.[Source: Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "\n",
    "Term frequency (`tf`) in `tf-idf` is the word frequency we get from above dictionary (`counts`). Now we need to calculate the `idf` part. \n",
    "\n",
    "The inverse document frequency is a measure of how much information the word provides, i.e., if it's common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "idf(t, D) = log(\\frac{N_D}{N_t})\n",
    "\\end{equation*}\n",
    "\n",
    "In which, $ N_D $ is the number of documents (`doc`) in `corpus` D; and $ N_t $ is the number of `doc`s in $ D $ containing term $ t $.\n",
    "\n",
    "Looks complicated, right? Fortunately we can use TextaCy's built-in methods to calculate `idf`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = corpus.word_doc_freqs(as_strings=True, weighting='idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(idf.items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since now we have both __tf__ as a dict object `counts`; and idf as a dict object `idf`, we can calculate the complete __tf-idf__ metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = {k: counts[k]/idf[k] for k in idf.keys() & counts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "Please print out the top-20 terms with the highest `tf-idf` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Complete your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of above results make sense, such as 'president' and 'bill' and 'act'. But terms like '-PRON-' (referring to pronouns such as 'you' or 'I') and ''s' do not make sense. Similar conclusion can be drawn onto words such as 'a', 'an', 'the', ...\n",
    "\n",
    "These words are called __stop words__. In computing, stop words are words which are filtered out before or after processing of natural language data (text). Though \"stop words\" usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these stop words to support phrase search. [Source: Wikipedia](https://en.wikipedia.org/wiki/Stop_words).\n",
    "\n",
    "We can check if a word (a.k.a. token) is stop by using the `is_stop` attribute provided with `textacy.token` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_doc = corpus[0]\n",
    "\n",
    "for t in my_doc.tokens:\n",
    "    print(t, t.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "Remove all stop words from `my_doc`.\n",
    "\n",
    "__HINT__: using an `if` statement (with the `is_stop` attribute) in the `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Complete your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we only care about __word tokens__, which means we need to filter out _numbers_, _punctuations_, and so forth. \n",
    "\n",
    "TextaCy provides a `is_alpha` attribute for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in my_doc.tokens:\n",
    "    print(t, t.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "What if we want non-stop and word tokens?\n",
    "\n",
    "__HINT__: combine the two above steps together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Complete you code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, have you noticed that different forms of the same token may appear in the text? For instance, 'run', 'ran', and 'running' are all different forms of the root word 'run'. Counting them as different words may bias any subsequent model. Thus, it would be ideal to make different forms of the same word to the root. This process is called __lemmatization__.\n",
    "\n",
    "_Lemmatization_ usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the _lemma_. If confronted with the token saw, stemming might return just s, whereas lemmatization would attempt to return either see or saw depending on whether the use of the token was as a verb or a noun. The two may also differ in that stemming most commonly collapses derivationally related words, whereas lemmatization commonly only collapses the different inflectional forms of a lemma. Linguistic processing for stemming or lemmatization is often done by an additional plug-in component to the indexing process, and a number of such components exist, both commercial and open-source. [Source: Stanford NLP Group](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "TextaCy provides a `lemma_` attribute for `token` object for exactly this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in my_doc.tokens:\n",
    "    print(t, t.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "In this exercise, you are going to complete following tasks.\n",
    "1. From the `corpus` variable, generate a new list named `jb_lst` that contains all speeches from `Joseph Biden`. (__HINT__: use similar code as we filter `cw` for 'Bernie Sanders'.)\n",
    "2. Select first 5 speeches (`doc`) from `jb_lst` and store them in a new list named `jb_selected`.\n",
    "3. For each element in `jb_selected`, print out:\n",
    "    a. Named Entities (`named_entities()`)\n",
    "    b. Text Statistics (`TextStats()`)\n",
    "4. For each element in `jb_selected`, print out the top 20 words based on their tf-idf score.\n",
    "5. For each element in `jb_selected`, print out __lemmas__ (`lemma_`) for each token if it is not a stop word (`is_stop`) and is a word token (`is_alpha`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Complete your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of part 1. We will resume on text analytics (NLP) after the break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
